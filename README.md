# Adversarial_Attackks-Defences_in_DNN
# Adversarial Attacks and Defenses in Deep Learning: A Research Work 
Welcome! This project explores how machine learning models can be tricked using adversarial attacks and how we can protect them using defense strategies. Think of it as a digital version of a sneaky trick and how to prevent it!

#  About the Research paper & Project
This experiment was designed to understand how artificial intelligence (AI) models can be fooled by adding tiny changes to images that humans can’t notice—but the model gets confused. To solve this, we also tested ways to protect these models and help them make better decisions even under tricky conditions. Beside this the research paper was submitted at IEEE Conference ICTBIG-2024


#  Author
I’m Vishakha, the primary author of this experiment. I’m currently pursuing BTech in Artificial Intelligence and Data Science and have a strong interest in AI safety, machine learning, and ethical AI development.

# What Was Done
Tested simple image-based attacks on AI models  
Applied basic defense techniques to reduce the impact of those attacks  
Compared how well the model performed before and after using defenses
