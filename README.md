# Adversarial_Attackks-Defences_in_DNN
# Adversarial Attacks and Defenses in Deep Learning: A Research Work 
Welcome! This project explores how machine learning models can be tricked using adversarial attacks and how we can protect them using defense strategies. Think of it as a digital version of a sneaky trick and how to prevent it!

✨ About the Project
This experiment was designed to understand how artificial intelligence (AI) models can be fooled by adding tiny changes to images that humans can’t notice—but the model gets confused. To solve this, we also tested ways to protect these models and help them make better decisions even under tricky conditions.


👩‍💻 Author
I’m Vishakha, the primary author of this experiment. I’m currently pursuing BTech in Artificial Intelligence and Data Science and have a strong interest in AI safety, machine learning, and ethical AI development.

🧪 What Was Done
Tested simple image-based attacks on AI models

Applied basic defense techniques to reduce the impact of those attacks

Compared how well the model performed before and after using defenses

📂 Files Included
main.py: The core code used to run the attacks and defenses

requirements.txt: List of packages you need to install before running the code

results/: Folder containing test results and images

README.md: You're reading it! 😉

▶️ How to Run
Install the required packages:

bash
Copy
Edit
pip install -r requirements.txt
Run the main code:

bash
Copy
Edit
python main.py
Check the results folder to see how the model reacted to the attacks and defenses.

🤝 Acknowledgements
This project was presented as part of the Ali Tripoli Conference ICT Bigfoot Corporate. Special thanks to everyone who contributed to the discussion and feedback!
